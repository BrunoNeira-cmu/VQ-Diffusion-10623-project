{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline FID and IS inception score using VQ-Difussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference_VQ_Diffusion import VQ_Diffusion\n",
    "# Perform Imports\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torcheval.metrics as metrics\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms.functional as TF\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from ignite.engine import *\n",
    "from ignite.handlers import *\n",
    "from ignite.metrics import *\n",
    "from ignite.metrics.regression import *\n",
    "from ignite.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some preliminary setup to load the dataset\n",
    "batch_size  = 256\n",
    "d_proj      = 512\n",
    "d_hidden    = 32\n",
    "config_path = \"logs/vqgan_gumbel_f8/configs/model.yaml\"\n",
    "chkpt_path = \"logs/vqgan_gumbel_f8/checkpoints/last.ckpt\"\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.PILToTensor(), # Convert all PIL objects to tensors\n",
    "    T.Resize((d_hidden, d_hidden)),  # Resize all images to d_h x d_h\n",
    "    T.ConvertImageDtype(torch.float),\n",
    "])\n",
    "\n",
    "# Define a custom collate_fn\n",
    "def custom_collate_fn(batch):\n",
    "    images, captions = zip(*batch)\n",
    "\n",
    "    # Find the max height and width in the batch\n",
    "    max_height = max(img.shape[1] for img in images)\n",
    "    max_width  = max(img.shape[2] for img in images)\n",
    "\n",
    "    # Pad images to the max dimensions\n",
    "    padded_images = [\n",
    "        F.pad(img, (0, max_width - img.shape[2], 0, max_height - img.shape[1]))\n",
    "        for img in images\n",
    "    ]\n",
    "\n",
    "    # Stack images and return with captions\n",
    "    return torch.stack(padded_images), captions\n",
    "\n",
    "# Define helper functions\n",
    "def normalize_output(x):\n",
    "  x = torch.clamp(x, -1., 1.)\n",
    "  x = (x + 1.)/2.\n",
    "  return x\n",
    "\n",
    "dataset = datasets.CocoCaptions(root = 'TEST_ROOT_PATH',\n",
    "                                annFile = 'TEST_ANN_FILE',\n",
    "                                transform=transform)\n",
    "# load the dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn = custom_collate_fn)\n",
    "\n",
    "# Load the model - coco pretrained\n",
    "VQ_Diffusion_model = VQ_Diffusion(config='OUTPUT/pretrained_model/config_text.yaml', path='OUTPUT/pretrained_model/coco_learnable.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncation_rate = 0.86\n",
    "\n",
    "FID = metrics.FrechetInceptionDistance()\n",
    "IS = InceptionScore()\n",
    "\n",
    "\n",
    "for (img, txt) in tqdm(dataloader):\n",
    "    batch_size = img.shape[0]\n",
    "\n",
    "    data_i = {}\n",
    "    data_i['label'] = [txt]\n",
    "    data_i['image'] = None\n",
    "\n",
    "    condition = txt\n",
    "\n",
    "    with torch.no_grad():\n",
    "            model_out = VQ_Diffusion_model.model.generate_content(\n",
    "                batch=data_i,\n",
    "                filter_ratio=0,\n",
    "                replicate=batch_size,\n",
    "                content_ratio=1,\n",
    "                return_att_weight=False,\n",
    "                sample_type=\"top\"+str(truncation_rate)+'r',\n",
    "            ) # B x C x H x W\n",
    "\n",
    "    content = model_out['content']\n",
    "\n",
    "    FID.update(images=img, is_real=True)\n",
    "    FID.update(images=content, is_real=False)\n",
    "\n",
    "    IS.update(output=content)\n",
    "\n",
    "fid_score = FID.compute()\n",
    "print(\"FID Score: \", fid_score)\n",
    "\n",
    "# calculate the FID score\n",
    "is_score = IS.compute()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
